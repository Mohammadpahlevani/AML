# -*- coding: utf-8 -*-
"""Hamshahri.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MBCTV318gL8eZuv164d5knnqlgJVABxp
"""

pip install hazm

# Import library
import re
import pandas as pd
import numpy as np
from hazm import *
import hazm
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
import math
import nltk
from sklearn.feature_extraction.text import CountVectorizer
from collections import defaultdict

!gdown --id 1D3yt99D0GcCRCbdKbUQGxbqjkeh91hTg

!unrar e hamshahri.rar

!unzip /content/Hamshahri-Corpus.zip

import pandas as pd
txt = open("/content/Hamshahri-Corpus.txt", "r", encoding="utf-8").read()
split_txt = txt.split('.DID')
del split_txt[0]
df = pd.DataFrame(columns=["date", "cat", "article"])
for i in range(1000):
    text = split_txt[i].split('\n')
    dateString = text[1]
    date = dateString.split('\t')[1]
    catString = text[2]
    cat = catString.split('\t')[1]
    article = ''.join(text[3:])
    df.loc[len(df)] = [date, cat, article]

print(df[["date", "cat", "article"]])

import matplotlib.pyplot as plt

# Bar plot
plt.figure(figsize=(10, 6))
df['cat'].value_counts().plot(kind='bar')
plt.title('Category Distribution')
plt.xlabel('Category')
plt.ylabel('Count')
plt.show()

# Line plot
plt.figure(figsize=(10, 6))
df['date'] = pd.to_datetime(df['date'], format='%y\\%m\\%d')
df.groupby('date').size().plot()
plt.title('Article Count Over Time')
plt.xlabel('Date')
plt.ylabel('Count')
plt.show()

# Scatter plot
plt.figure(figsize=(10, 6))
df['word_count'] = df['article'].apply(lambda x: len(x.split()))
plt.scatter(df['date'], df['word_count'], alpha=0.5)
plt.title('Article Word Count Over Time')
plt.xlabel('Date')
plt.ylabel('Word Count')
plt.show()

df.info()

norm = Normalizer()
tokenizer = WordTokenizer()
stopword = stopwords_list()
stemm = Stemmer()
def preproc(text):
    norm_text = norm.normalize(text)
    tok = tokenizer.tokenize(norm_text)
    tok = [token for token in tok if token not in stopword]
    stem = [stemm.stem(token) for token in tok]
    preproc_text = ' '.join(stem)
    return preproc_text

df['new_article'] = df['article'].apply(preproc)

df = df.drop(['article'], axis=1)

df.head()

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()

tfidf_matrix = vectorizer.fit_transform(df['new_article'])

print(tfidf_matrix)

import re

df['date'] = pd.to_datetime(df['date'], format='%y\\%m\\%d')

time_intervals = df['date'].unique()

def count_word_occurrences(text, word):
    occurrences = re.findall(r'\b{}\b'.format(re.escape(word)), text)
    return len(occurrences)

word_frequency = []

for interval in time_intervals:
    text_subset = df[df['date'] <= interval]['new_article'].str.cat(sep='\n')
    frequency = count_word_occurrences(text_subset, 'ناتو')
    word_frequency.append(frequency)

plt.plot(time_intervals, word_frequency)
plt.xlabel('Time Intervals')
plt.ylabel('Frequency of "ناتو"')
plt.title('Frequency of "ناتو" over Time')
plt.xticks(rotation=45)
plt.show()

!pip install -U scikit-learn

from sklearn.decomposition import PCA

pca = PCA(n_components=2)

reduced_data = pca.fit_transform(tfidf_matrix.toarray())

print(reduced_data.shape)

import matplotlib.pyplot as plt

x = reduced_data[:, 0]
y = reduced_data[:, 1]

plt.scatter(x, y)
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.title('Data Visualization in 2 Dimensions')
plt.show()

from sklearn.cluster import KMeans

n_clusters = 5

kmeans = KMeans(n_clusters=n_clusters, random_state=0)

cluster_labels = kmeans.fit_predict(reduced_data)

print(cluster_labels)

import matplotlib.pyplot as plt

plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=cluster_labels)
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.title('Clustering Results')
plt.show()

from sklearn.decomposition import PCA

pca = PCA(n_components=5)

reduced_data_5d = pca.fit_transform(tfidf_matrix.toarray())

print(reduced_data_5d.shape)

from sklearn.cluster import KMeans

n_clusters = 5

kmeans = KMeans(n_clusters=n_clusters, random_state=0)

cluster_labels_5d = kmeans.fit_predict(reduced_data_5d)

df['cluster_labels'] = cluster_labels_5d

df.to_csv('final_result.csv', index=False)